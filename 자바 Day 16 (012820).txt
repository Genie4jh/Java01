**HTML

1. Tag: 브라우저가 해석해서 렌더링하기 위한 명령어
=> 하나의 페이지에서 중복될 수 있습니다 .

2. class: 동일한 디자인을 적용하기 위해 태그에 적용하는 속성
=> 하나의 페이지에서 중복될 수 있습니다. 

3. id: 자바스크립트에서 하나의 태그를 구분하기 위해서 적용하는 속성
=> 하나의 페이지에서는 절대로 중복될 수 없습니다. 

4. name: 서버에서 처리하기 위해 붙이는 이름
=> 중복될 수 있습니다. 

5. attribute: 태그 내에서 설정하는 옵션으로 key-valeu 형태로 설정
<태그옵션이름='값'>: 스크래핑을 할 때는 a 태그의 href 속성이 가장 중요

6. selector: 별도로 설정하는 것이 아니고 css 에서 태그들을 선택하기 위해 만든 문법
태그
.클래스이름
#id

7. xpath: 별도로 설정하는 것이 아니고 브라우저가 태그의 경로를 나타내기 위해서 사용하는 문법
태그의 경로를 구분해서 나타내기 떄문에 중복될 수 없습니다. 



**java가 실행을 할 때 클래스를 찾는 방법
1. 자신의 애플리케이션에 찾습니다.
2. 1번에서 없으면 jvm이 제공하는 라이브러리에서 찾습니다.
3. 자바 실행을 할 때 자신의 build path에 추가된 라이브러리를 jvm에 로드를 합니다. 
2번에서도 없으면 build path에 추가된 라이브러리에서 찾습니다. 
=> build path에 추가될 때는 링크가 설정됩니다. 
=> 외부 라이브러리를 사용할 때는 프로젝트에서 포함해서 배포를 하던가 다운로드 링크를 같이 제공해야 합니다. 



**HTML Parsing
=> Java에서는 Python의 BeautifulSoup 과 유사한 JSoup 이라는 라이브러리를 이용

1. https://www.naver.com 에서 원하는 항목의 값 가져오기
1) 프로젝트의 build path에 jsoup 라이브러리를 추가
=> www.mnvrepository.com에서 jsoup 을 검색해서 다운로드
=> 다운로드 받은 파일을 프로젝트에 복사
=> 복사한 파일을 선택해서 마우스 오른쪽을 클릭하고 [Build Path] - [Add To Build Path]를 실행

public class NaverMain {

	public static void main(String[] args) {		
		// 필요한 HTML을 다운로드: https://www.naver.com 에서 다운로드	
		String html = null;
		try {
			// 다운롣 받을 URL을 생성
			// URL 에 한글이 있으면 URLEncoder.encode 이용해서 인코딩을 해야 합니다. 
			String addr = "https://www.naver.com";
			URL url = new URL(addr);
			
			// 연결
			HttpURLConnection con = (HttpURLConnection)url.openConnection();
			con.setConnectTimeout(30000);
			con.setUseCaches(false);
			
			// 데이터를 다운로드
			StringBuilder sb = new StringBuilder();
			BufferedReader br = new BufferedReader(
					new InputStreamReader(
							con.getInputStream()));
			
			while(true) {
				String line = br.readLine();
				if(line == null) {
					break;
				}
				sb.append(line + "\n");
			}
			
			// 사용한 스트림 정리
			br.close();
			con.disconnect();
			// 데이터 확인
			html = sb.toString();
			System.out.println(html);
					
			
		}catch(Exception e) {
			System.out.println("다운로드 예외:" + e.getMessage());
			e.printStackTrace();
		}
		
		if(html == null) {
			System.out.println("다운로드 받은 문자열이 없습니다.");
			return;
		}
		
		// HTML 파싱을 해서 원하는 자료구조 만들기
		try {
			// 문자열을 메모리에 DOM(Document Object Model)으로 펼치기
			Document document = Jsoup.parse(html);
			
			/*
			// span 태그 가져오기 - 태그는 중복될 수 있으므로 여러 개 리턴
			Elements span = document.getElementsByTag("span");
			// 가져온 데이터를 순회하면서 출력
			for(int i=0; i < span.size(); i=i+1) {
				Element element = span.get(i);
				System.out.println(element.text());
			}
			*/
			
			/*
			// 클래스 이름을 이용해서 찾아오기
			Elements span = document.getElementsByClass("an_txt");
			// 가져온 데이터를 순회하면서 출력
			for(int i=0; i < span.size(); i=i+1) {
				Element element = span.get(i);
				System.out.println(element.text());
			}
			*/
			
			// 선택자 이용
			Elements span = document.select("#PM_ID_serviceNavi > li:nth-child(9) > a > span.an_txt");
			// 가져온 데이터를 순회하면서 출력
			for(int i=0; i < span.size(); i=i+1) {
				Element element = span.get(i);
				System.out.println(element.text());
			}
			
			
		}catch(Exception e) {
			System.out.println("파싱 예외:" + e.getMessage());
			e.printStackTrace();
		}

	}

}


2. 한겨레 신문사에서 검색을 해서 실제 기사를 전부 읽어서 파일에 저장하기
=> 검색을 이용해서 데이터를 읽어올 때는 주소 패턴을 정확하게 이해


keyword 가 검색어
datefrom 이 시작날짜 dateto 가 종료 날짜
pageseq 가 페이지 번호 - 0부터 시작

=> 기사 개수를 읽어오는 것이 먼저 - 반복문을 언제까지 돌려하는지 설정

=> 기사 검색이나 게시판, sns 검색의 경우 검색의 결과로 나온 것은 대부분 제목과 본문의 링크입니다.
검색의 결과에서 링크를 전부 찾아서 실제 기사나 본문의 내용을 읽어와야 합니다. 

public class HaniMain {

	public static void main(String[] args) {
		// 한겨레 신문사 웹 페이지에서 코로나 로 검색된 기사 내용을 전부 파일에 저장하기
		
		// 1. 코로나로 검색된 기사 개수 파악하기
		// 기사 개수를 저장할 변수
		int cnt = 0;
		
		try {
			// 한글로 된 검색어를 인코딩
			String keyword  = URLEncoder.encode("코로나", "utf-8");
			// 한겨레 신문사 ㄴ ㅠ스 검색 URL 만들기
			String addr = "http://search.hani.co.kr/Search?command=query&keyword=" 
					+ keyword 
					+ "&media=news&submedia=&sort=d&period=all"
					+ "&datafrom=2000.01.01&dateto=2020.01.28&pageseq=0";
			URL url = new URL(addr);
			
			// 연결 옵션과 연결
			HttpURLConnection con = (HttpURLConnection)url.openConnection();
			con.setConnectTimeout(20000);
			con.setUseCaches(false);
			
			
			// 문자열을 전부 읽어서 sb 에 저장하기
			StringBuilder sb = new StringBuilder();
			BufferedReader br = new BufferedReader(
					new InputStreamReader(con.getInputStream()));
			
			while(true) {
				String line = br.readLine();
				if(line == null) {
					break;
				}
				sb.append(line + "\n");
			}
			String html = sb.toString();
			br.close();
			con.disconnect();
			// 데이터 확인 - 확인되면 주석 처리
			//System.out.println(html);
			
			// html 을 DOM 으로 펼치기
			Document document = Jsoup.parse(html);
			
			// 태그 이름으로 찾기
			//Elements spans = document.getElementsByTag("span");
			
			// 클래스이름으로 찾기
			//Elements spans = document.getElementsByClass("total");
			
			// 선택자를 이용해서 찾기
			Elements spans = document.select(
					"#contents > div.search-result-section.first-child > div > span");
			
			String temp = spans.get(0).text();
			// 240 건 이라고 temp 에 저장; 240 숫자만 추출해서 정수로 변환해서 cnt에 저장
			// 공백으로 분할한 후 첫번째 데이터를 정수로 변환해서 cnt에 저장
			String [] ar = temp.split(" ");
			cnt = Integer.parseInt(ar[0]);
			//System.out.println(cnt);
			
			
		}catch(Exception e) {
			System.out.println("기사 개수 파악 예외:" + e.getMessage());
			e.printStackTrace();
		}
		
		if(cnt <= 0) {
			System.out.println("검색된 기사가 없습니다.");
			return;
		}
		
		// 검색된 데이터의 링크를 전부 찾아서 list 에 삽입
		ArrayList <String> list = new ArrayList<String>();
		try {
			// 페이지 개수 구하기
			int pagesu = cnt / 10;
			if(pagesu % 10 != 0) {
				pagesu = pagesu + 1;
			}
			
			// 페이지 단위로 순회
			for(int i=0; i<pagesu; i=i+1) {
				// 한글로 된 검색어를 인코딩
				String keyword  = URLEncoder.encode("코로나", "utf-8");
				// 한겨레 신문사 ㄴ ㅠ스 검색 URL 만들기
				String addr = "http://search.hani.co.kr/Search?command=query&keyword=" 
						+ keyword 
						+ "&media=news&submedia=&sort=d&period=all"
						+ "&datafrom=2000.01.01&dateto=2020.01.28&pageseq=0" + i;
				URL url = new URL(addr);
				
				HttpURLConnection  con = (HttpURLConnection)url.openConnection();
				con.setConnectTimeout(10000);
				con.setUseCaches(false);
				
				// 특정 페이지의 데이터를 읽지 못하더라도 다음 페이지의 데이터를 읽기 위해서 반복문안에 예외처리문장 삽입
				try {
					StringBuilder sb = new StringBuilder();
					BufferedReader br = new BufferedReader(
							new InputStreamReader(
									con.getInputStream()));
					
					while(true) {
						String line = br.readLine();
						if(line == null) {
							break;
						}
						sb.append(line + "\n");
					}
					
					String html = sb.toString();
					br.close();
					con.disconnect();
					
					Document document = Jsoup.parse(html);
					Elements links = document.select("dl > dt > a");
					for(int j=0; j<links.size(); j=j+1) {
						// a 태그 안에 있는 문자열을 가져오기
						//System.out.println(links.get(j).text());
						
						// a 태그의 link 가져오기
						//System.out.println(links.get(j).attr("href"));
						list.add(links.get(j).attr("href"));
					}
					//System.out.println(list.size());
					
				}catch(Exception e) {
					System.out.println("링크 수집 예외:" + e.getMessage());
					e.printStackTrace();
				}
			}
			
		}catch(Exception e) {
			System.out.println("링크 수집 예외:" + e.getMessage());
			e.printStackTrace();
		}
		
		// list 에 저장된 링크의 기사 데이터를 전부 읽어서 파일에 저장하기
		try {
			for(String addr : list) {
				try {
					URL url = new URL(addr);
					HttpURLConnection con = (HttpURLConnection)url.openConnection();
					con.setUseCaches(false);
					con.setConnectTimeout(10000);
					
					StringBuilder sb = new StringBuilder();
					BufferedReader br = new BufferedReader(
							new InputStreamReader(
									con.getInputStream()));
					
					while(true) {
						String line = br.readLine();
						if(line == null) {
							break;
						}
						sb.append(line + "\n");
					}
					String html = sb.toString();
					br.close();
					con.disconnect();
					//System.out.println(html);
					
					// 파일에 기록
					Document document = Jsoup.parse(html);
					Elements articles = document.getElementsByClass("title");
					// 찾아온 데이터를 파일에 기록
					for(int i=0; i<articles.size(); i=i+1) {
						PrintWriter pw = new PrintWriter(
								new FileOutputStream("./코로나.txt", true));
						pw.println(articles.get(i).text());
						pw.flush();
						pw.close();
					}
					
					// 실제 여러 개의 페이지에서 스크래핑 할 때, 딜레이를 주는 것이 좋습니다. 
					//Thread.sleep(1000);
					
				}catch(Exception e) {
					System.out.println("기사 읽기 예외:" + e.getMessage());
					e.printStackTrace();
				}
			}
			
		}catch(Exception e) {
			System.out.println("링크 수집 예외:" + e.getMessage());
			e.printStackTrace();
		}

	}

}





